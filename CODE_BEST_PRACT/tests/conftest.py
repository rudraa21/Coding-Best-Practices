import os
import sys
#setting path
current = os.path.dirname(os.path.realpath(__file__))
parent = os.path.dirname(current)
sys.path.append(current)
sys.path.append(parent)

# importing necessary functions
from pyspark.sql.session import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from pytest import fixture


@fixture(scope = 'session')
def spark() -> SparkSession:
    return SparkSession.builder.getOrCreate() 

@fixture(scope = 'session')
def dbutils(spark: SparkSession):
    try:
        from pyspark.dbutils import DBUtils
        dbutils = DBUtils(spark)
    except ImportError:
        import IPython
        dbutils = IPython.get_ipython().user_ns.get("dbutils")
        if not dbutils:
            log.warning("could not initialise dbutils!")
    return dbutils

@fixture
def create_mock_dataframe(spark):
    """
    Fixture to create a PySpark DataFrame
    Arguments:
    - data (Optional[List[Tuple]]): Data to populate the DataFrame. If not provided, an empty DataFrame is created.
    - schema (Optional[StructType]): Schema for the DataFrame. If not provided, a default schema is used.

    Returns:
    - DataFrame: PySpark DataFrame.
    """
    def _create_mock_dataframe(data, schema=None):
        if schema:
            # If schema is provided, create a DataFrame with the specified schema
            return spark.createDataFrame(data, schema)
        elif data:
            # If data is provided but no schema, create a DataFrame from the given data with an autogenerated schema
            columns = [f'col_{i}' for i in range(len(data[0]))]
            return spark.createDataFrame(data, columns)
        else:
            # If neither data nor schema is provided, create an empty DataFrame with an explicit schema
            default_schema = StructType([StructField("col_0", StringType(), True)])
            return spark.createDataFrame([], default_schema)
    return _create_mock_dataframe

def are_dataframes_equal(df1, df2):
    """
    Check if two PySpark DataFrames match based on data alone.

    The function compares the content of the DataFrames disregarding column names
    and order. It returns True if the DataFrames have the same data and False
    otherwise.

    Parameters:
    - df1 (pyspark.sql.DataFrame): The first PySpark DataFrame for comparison.
    - df2 (pyspark.sql.DataFrame): The second PySpark DataFrame for comparison.

    Returns:
    - bool: True if the DataFrames match based on data alone, False otherwise.
    """

    # if both dataframe have no records, returning true
    if df1.isEmpty() and df2.isEmpty():
        return True

    # Get the difference between the two DataFrames
    difference_df = df1.exceptAll(df2).union(df2.exceptAll(df1)).distinct()

    # Check if there are any differences
    if difference_df.isEmpty():
        return True  # DataFrames match based on data
    else:
        print("DataFrames do not match. Differences:")
        difference_df.show()
        print(difference_df.count())
        return False  # DataFrames do not match based on data   
